{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Build The Dataset\n",
    "\n",
    "#### Set up the key by using a parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# Set up the key\n",
    "import os\n",
    "import configparser\n",
    "# Use a parser for the configuration file\n",
    "import pandas as pd\n",
    "\n",
    "configs = configparser.ConfigParser()\n",
    "# Get the current directory to the main file README.md\n",
    "currentDir = os.path.dirname(\"README.md\")\n",
    "# Get the path file to the config file\n",
    "configDir = os.path.join(currentDir, \"config/settings.cfg\")\n",
    "configs.read(configDir)\n",
    "# Get the key\n",
    "apiKey = configs.get(\"nytimes\", \"api_key\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Find total numbers of articles in the topic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import requests\n",
    "subject = \"subject:Asian-Americans\"\n",
    "query = f'https://api.nytimes.com/svc/search/v2/articlesearch.json?fq={subject}&api-key={apiKey}'\n",
    "response = requests.get(query).json()\n",
    "numHits = response['response']['meta']['hits']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Structure the query and parse the file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# Method to set up and parse the\n",
    "numPerPages = 10\n",
    "import time\n",
    "import dateutil\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def getData() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main method to send request and parse response with the subject\n",
    "\n",
    "    :return: Dataset with all the articles parsed as panda dataframe\n",
    "    \"\"\"\n",
    "    # Result dataset\n",
    "    dataset = {'headline': [],\n",
    "        'date': [],\n",
    "        'doc_type': [],\n",
    "        'author': [],\n",
    "        'section': [],\n",
    "        'url': [],\n",
    "        'news_desk': [],\n",
    "        'material_type': [],\n",
    "        'word_count': [],\n",
    "        'keywords': []}\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    # Count number of articles\n",
    "    total = 0\n",
    "    # Loop through the page\n",
    "    for page in range(numHits // numPerPages):\n",
    "        # Send request to the page gradually\n",
    "        q = f'https://api.nytimes.com/svc/search/v2/articlesearch.json?fq={subject}&page={page}&api-key={apiKey}'\n",
    "        r = requests.get(q).json()\n",
    "        # Based on preliminary parsing, get the list of article from the file\n",
    "        articleList = r['response']['docs']\n",
    "        # Return dataframe\n",
    "        frame = getRequest(articleList)\n",
    "        # Add to dataset\n",
    "        dataset = dataset.append(frame, ignore_index=True)\n",
    "        # Count pages\n",
    "        total += len(frame)\n",
    "        # Sleep before new request\n",
    "        time.sleep(6)\n",
    "        # Print message to know finish with the page\n",
    "        print(\"Finish with page\", page)\n",
    "    # Print when done with the numbers of total articles\n",
    "    print(f'Finished with all pages, total of {str(total)}')\n",
    "    # Create csv file\n",
    "    csv_path = \"Asians American NYT Dataset.csv\"\n",
    "    dataset.to_csv(csv_path, index=False)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def isNotValid(article) -> bool:\n",
    "    \"\"\"\n",
    "    Method to check if the article has a valid headline\n",
    "\n",
    "    :param article: The information of the article\n",
    "    :return: True if not valid, False if valid\n",
    "    \"\"\"\n",
    "    if type(article['headline']) == dict and 'main' in article['headline'] and article['headline']['main'] is not None:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def getRequest(articleList) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Method to parse article and return as a data frame\n",
    "\n",
    "    :param articleList: list of article from response['response']['docs']\n",
    "    :return: dataframe of the article after parsing\n",
    "    \"\"\"\n",
    "    frame = {'headline': [],\n",
    "        'date': [],\n",
    "        'doc_type': [],\n",
    "        'author': [],\n",
    "        'section': [],\n",
    "        'url': [],\n",
    "        'news_desk': [],\n",
    "        'material_type': [],\n",
    "        'word_count': [],\n",
    "        'keywords': []}\n",
    "    for article in articleList:\n",
    "        # Check if article is valid\n",
    "        if isNotValid(article):\n",
    "            continue\n",
    "        # Date parse\n",
    "        date = dateutil.parser.parse(article['pub_date']).date()\n",
    "        frame['date'].append(date)\n",
    "        # Headline parse\n",
    "        frame['headline'].append(article['headline']['main'])\n",
    "        # Document type parse\n",
    "        frame['doc_type'].append(article['document_type'])\n",
    "        # Author parse\n",
    "        if 'person' in article['byline']:\n",
    "            person = article['byline']['person']\n",
    "            names = [name['firstname'] + \" \" + name['lastname'] for name in person if name['firstname'] and name['lastname']]\n",
    "            frame['author'].append(names)\n",
    "        else:\n",
    "            frame['author'].append(None)\n",
    "        # Section parse\n",
    "        if 'section' in article:\n",
    "            frame['section'].append(article['section_name'])\n",
    "        else:\n",
    "            frame['section'].append(None)\n",
    "        # Link URL parse\n",
    "        if 'web_url' in article:\n",
    "            frame['url'].append(article['web_url'])\n",
    "        else:\n",
    "            frame['url'].append(None)\n",
    "        # News Desk parse\n",
    "        if 'news_desk' in article:\n",
    "            frame['news_desk'].append(article['news_desk'])\n",
    "        else:\n",
    "            frame['news_desk'].append(article[None])\n",
    "        # Material parse:\n",
    "        if 'type_of_material' in article:\n",
    "            frame['material_type'].append(article['type_of_material'])\n",
    "        else:\n",
    "            frame['material_type'].append(None)\n",
    "        # Word count parse\n",
    "        if 'word_count' in article:\n",
    "            frame['word_count'].append(article['word_count'])\n",
    "        else:\n",
    "            frame['word_count'].append(article[None])\n",
    "        # Keyword parse\n",
    "        keywords = [(keyword['name'], keyword['value']) for keyword in article['keywords']]\n",
    "        frame['keywords'].append(keywords)\n",
    "\n",
    "    return pd.DataFrame(frame)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build the dataset into a csv file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish with page 0\n",
      "Finish with page 1\n",
      "Finish with page 2\n",
      "Finish with page 3\n",
      "Finish with page 4\n",
      "Finish with page 5\n",
      "Finish with page 6\n",
      "Finish with page 7\n",
      "Finish with page 8\n",
      "Finish with page 9\n",
      "Finish with page 10\n",
      "Finish with page 11\n",
      "Finish with page 12\n",
      "Finish with page 13\n",
      "Finish with page 14\n",
      "Finish with page 15\n",
      "Finish with page 16\n",
      "Finish with page 17\n",
      "Finish with page 18\n",
      "Finish with page 19\n",
      "Finish with page 20\n",
      "Finish with page 21\n",
      "Finish with page 22\n",
      "Finish with page 23\n",
      "Finish with page 24\n",
      "Finish with page 25\n",
      "Finish with page 26\n",
      "Finish with page 27\n",
      "Finish with page 28\n",
      "Finish with page 29\n",
      "Finish with page 30\n",
      "Finish with page 31\n",
      "Finish with page 32\n",
      "Finish with page 33\n",
      "Finish with page 34\n",
      "Finish with page 35\n",
      "Finish with page 36\n",
      "Finish with page 37\n",
      "Finish with page 38\n",
      "Finish with page 39\n",
      "Finish with page 40\n",
      "Finish with page 41\n",
      "Finish with page 42\n",
      "Finish with page 43\n",
      "Finish with page 44\n",
      "Finish with page 45\n",
      "Finish with page 46\n",
      "Finish with page 47\n",
      "Finish with page 48\n",
      "Finish with page 49\n",
      "Finish with page 50\n",
      "Finish with page 51\n",
      "Finish with page 52\n",
      "Finish with page 53\n",
      "Finish with page 54\n",
      "Finish with page 55\n",
      "Finish with page 56\n",
      "Finish with page 57\n",
      "Finish with page 58\n",
      "Finish with page 59\n",
      "Finish with page 60\n",
      "Finish with page 61\n",
      "Finish with page 62\n",
      "Finish with page 63\n",
      "Finish with page 64\n",
      "Finish with page 65\n",
      "Finish with page 66\n",
      "Finish with page 67\n",
      "Finish with page 68\n",
      "Finish with page 69\n",
      "Finish with page 70\n",
      "Finish with page 71\n",
      "Finish with page 72\n",
      "Finish with page 73\n",
      "Finish with page 74\n",
      "Finish with page 75\n",
      "Finish with page 76\n",
      "Finish with page 77\n",
      "Finish with page 78\n",
      "Finish with page 79\n",
      "Finish with page 80\n",
      "Finish with page 81\n",
      "Finish with page 82\n",
      "Finish with page 83\n",
      "Finish with page 84\n",
      "Finish with page 85\n",
      "Finish with page 86\n",
      "Finish with page 87\n",
      "Finish with page 88\n",
      "Finish with page 89\n",
      "Finish with page 90\n",
      "Finish with page 91\n",
      "Finish with page 92\n",
      "Finish with page 93\n",
      "Finish with page 94\n",
      "Finish with page 95\n",
      "Finish with page 96\n",
      "Finish with page 97\n",
      "Finish with page 98\n",
      "Finish with page 99\n",
      "Finish with page 100\n",
      "Finish with page 101\n",
      "Finish with page 102\n",
      "Finish with page 103\n",
      "Finish with page 104\n",
      "Finish with page 105\n",
      "Finish with page 106\n",
      "Finish with page 107\n",
      "Finish with page 108\n",
      "Finish with page 109\n",
      "Finish with page 110\n",
      "Finish with page 111\n",
      "Finish with page 112\n",
      "Finish with page 113\n",
      "Finish with page 114\n",
      "Finish with page 115\n",
      "Finish with page 116\n",
      "Finish with page 117\n",
      "Finish with page 118\n",
      "Finish with page 119\n",
      "Finish with page 120\n",
      "Finish with page 121\n",
      "Finish with page 122\n",
      "Finish with page 123\n",
      "Finish with page 124\n",
      "Finish with page 125\n",
      "Finish with page 126\n",
      "Finish with page 127\n",
      "Finish with page 128\n",
      "Finish with page 129\n",
      "Finish with page 130\n",
      "Finish with page 131\n",
      "Finish with page 132\n",
      "Finish with page 133\n",
      "Finish with page 134\n",
      "Finish with page 135\n",
      "Finish with page 136\n",
      "Finish with page 137\n",
      "Finish with page 138\n",
      "Finish with page 139\n",
      "Finish with page 140\n",
      "Finish with page 141\n",
      "Finish with page 142\n",
      "Finish with page 143\n",
      "Finish with page 144\n",
      "Finish with page 145\n",
      "Finish with page 146\n",
      "Finish with page 147\n",
      "Finish with page 148\n",
      "Finish with page 149\n",
      "Finish with page 150\n",
      "Finish with page 151\n",
      "Finish with page 152\n",
      "Finish with page 153\n",
      "Finish with page 154\n",
      "Finish with page 155\n",
      "Finish with page 156\n",
      "Finish with page 157\n",
      "Finish with page 158\n",
      "Finish with page 159\n",
      "Finish with page 160\n",
      "Finished with all pages, total of 1610\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                            headline        date doc_type  \\\n0  A Rising Star’s Career Was Cut Short. His Impa...  2021-07-19  article   \n1  Shohei Ohtani Is Just the Star America’s Pasti...  2021-07-12  article   \n2        Anti-Asian Attacks Continue as City Reopens  2021-07-14  article   \n3  Fear, and Discord, Among Asian Americans Over ...  2021-07-18  article   \n4  Boston Overhauls Admissions to Exclusive Exam ...  2021-07-15  article   \n5  ‘No Vaccine for Racism’: Asian New Yorkers Sti...  2021-07-14  article   \n6  A ‘Rogue Ballerina’ Gives a Candid Account of ...  2021-07-14  article   \n7  Violinist Apologizes for ‘Culturally Insensiti...  2021-06-28  article   \n8  After Years of Sex and Lies, David Choe Is Rea...  2021-06-24  article   \n9  Billie Eilish Apologizes for Lip-Syncing Anti-...  2021-06-22  article   \n\n                          author section  \\\n0              [Andrew LaVallee]    None   \n1                [Kurt Streeter]    None   \n2                  [Ashley Wong]    None   \n3                [Thomas Fuller]    None   \n4                  [Ellen Barry]    None   \n5  [Ali Watkins, Jonah Bromwich]    None   \n6                  [Gia Kourlas]    None   \n7             [Javier Hernández]    None   \n8                   [Edmund Lee]    None   \n9             [Anna Kambhampaty]    None   \n\n                                                 url     news_desk  \\\n0  https://www.nytimes.com/2021/07/19/books/antho...         Books   \n1  https://www.nytimes.com/2021/07/12/sports/base...        Sports   \n2  https://www.nytimes.com/2021/07/14/nyregion/at...         Metro   \n3  https://www.nytimes.com/2021/07/18/us/asian-at...      National   \n4  https://www.nytimes.com/2021/07/15/us/boston-s...      National   \n5  https://www.nytimes.com/2021/07/14/nyregion/as...         Metro   \n6  https://www.nytimes.com/2021/07/14/arts/dance/...  Arts&Leisure   \n7  https://www.nytimes.com/2021/06/28/arts/music/...       Culture   \n8  https://www.nytimes.com/2021/06/24/business/me...      Business   \n9  https://www.nytimes.com/2021/06/22/style/billi...        Styles   \n\n  material_type  word_count                                           keywords  \n0          News      1516.0  [(persons, So, Anthony Veasna (1992-2020)), (s...  \n1          News      1032.0  [(subject, Baseball), (organizations, Los Ange...  \n2      briefing      1114.0  [(glocations, New York City), (subject, Hate C...  \n3          News      1387.0  [(glocations, San Francisco (Calif)), (subject...  \n4          News      1480.0  [(glocations, Boston (Mass)), (subject, Educat...  \n5          News      1348.0  [(subject, Discrimination), (subject, Hate Cri...  \n6          News      1742.0  [(persons, Pazcoguin, Georgina), (subject, Dan...  \n7          News       577.0  [(subject, Discrimination), (subject, Classica...  \n8          News      1510.0  [(subject, Art), (subject, Television), (subje...  \n9          News       361.0  [(persons, Eilish, Billie), (subject, Asian-Am...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>headline</th>\n      <th>date</th>\n      <th>doc_type</th>\n      <th>author</th>\n      <th>section</th>\n      <th>url</th>\n      <th>news_desk</th>\n      <th>material_type</th>\n      <th>word_count</th>\n      <th>keywords</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A Rising Star’s Career Was Cut Short. His Impa...</td>\n      <td>2021-07-19</td>\n      <td>article</td>\n      <td>[Andrew LaVallee]</td>\n      <td>None</td>\n      <td>https://www.nytimes.com/2021/07/19/books/antho...</td>\n      <td>Books</td>\n      <td>News</td>\n      <td>1516.0</td>\n      <td>[(persons, So, Anthony Veasna (1992-2020)), (s...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Shohei Ohtani Is Just the Star America’s Pasti...</td>\n      <td>2021-07-12</td>\n      <td>article</td>\n      <td>[Kurt Streeter]</td>\n      <td>None</td>\n      <td>https://www.nytimes.com/2021/07/12/sports/base...</td>\n      <td>Sports</td>\n      <td>News</td>\n      <td>1032.0</td>\n      <td>[(subject, Baseball), (organizations, Los Ange...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Anti-Asian Attacks Continue as City Reopens</td>\n      <td>2021-07-14</td>\n      <td>article</td>\n      <td>[Ashley Wong]</td>\n      <td>None</td>\n      <td>https://www.nytimes.com/2021/07/14/nyregion/at...</td>\n      <td>Metro</td>\n      <td>briefing</td>\n      <td>1114.0</td>\n      <td>[(glocations, New York City), (subject, Hate C...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Fear, and Discord, Among Asian Americans Over ...</td>\n      <td>2021-07-18</td>\n      <td>article</td>\n      <td>[Thomas Fuller]</td>\n      <td>None</td>\n      <td>https://www.nytimes.com/2021/07/18/us/asian-at...</td>\n      <td>National</td>\n      <td>News</td>\n      <td>1387.0</td>\n      <td>[(glocations, San Francisco (Calif)), (subject...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Boston Overhauls Admissions to Exclusive Exam ...</td>\n      <td>2021-07-15</td>\n      <td>article</td>\n      <td>[Ellen Barry]</td>\n      <td>None</td>\n      <td>https://www.nytimes.com/2021/07/15/us/boston-s...</td>\n      <td>National</td>\n      <td>News</td>\n      <td>1480.0</td>\n      <td>[(glocations, Boston (Mass)), (subject, Educat...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>‘No Vaccine for Racism’: Asian New Yorkers Sti...</td>\n      <td>2021-07-14</td>\n      <td>article</td>\n      <td>[Ali Watkins, Jonah Bromwich]</td>\n      <td>None</td>\n      <td>https://www.nytimes.com/2021/07/14/nyregion/as...</td>\n      <td>Metro</td>\n      <td>News</td>\n      <td>1348.0</td>\n      <td>[(subject, Discrimination), (subject, Hate Cri...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>A ‘Rogue Ballerina’ Gives a Candid Account of ...</td>\n      <td>2021-07-14</td>\n      <td>article</td>\n      <td>[Gia Kourlas]</td>\n      <td>None</td>\n      <td>https://www.nytimes.com/2021/07/14/arts/dance/...</td>\n      <td>Arts&amp;Leisure</td>\n      <td>News</td>\n      <td>1742.0</td>\n      <td>[(persons, Pazcoguin, Georgina), (subject, Dan...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Violinist Apologizes for ‘Culturally Insensiti...</td>\n      <td>2021-06-28</td>\n      <td>article</td>\n      <td>[Javier Hernández]</td>\n      <td>None</td>\n      <td>https://www.nytimes.com/2021/06/28/arts/music/...</td>\n      <td>Culture</td>\n      <td>News</td>\n      <td>577.0</td>\n      <td>[(subject, Discrimination), (subject, Classica...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>After Years of Sex and Lies, David Choe Is Rea...</td>\n      <td>2021-06-24</td>\n      <td>article</td>\n      <td>[Edmund Lee]</td>\n      <td>None</td>\n      <td>https://www.nytimes.com/2021/06/24/business/me...</td>\n      <td>Business</td>\n      <td>News</td>\n      <td>1510.0</td>\n      <td>[(subject, Art), (subject, Television), (subje...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Billie Eilish Apologizes for Lip-Syncing Anti-...</td>\n      <td>2021-06-22</td>\n      <td>article</td>\n      <td>[Anna Kambhampaty]</td>\n      <td>None</td>\n      <td>https://www.nytimes.com/2021/06/22/style/billi...</td>\n      <td>Styles</td>\n      <td>News</td>\n      <td>361.0</td>\n      <td>[(persons, Eilish, Billie), (subject, Asian-Am...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = getData()\n",
    "data.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset is built, but the last page is not properly incorporated, so I add the method to get the final page manually.\n",
    "Changing the above code would mean rerun and wait for the dataset to repopulate, which is not something I want to do.\n",
    "Some thins I notice looking at the dataset:\n",
    "\n",
    "* Some lines in the dataset seem to only have a few words, then all blank\n",
    "* Some articles are repeated: differ with some words on the headlines; most of the links are the same, only differ by the number."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def getFinalPage(dataset):\n",
    "    # Send request to the page gradually\n",
    "    q = f'https://api.nytimes.com/svc/search/v2/articlesearch.json?fq={subject}&page=161&api-key={apiKey}'\n",
    "    r = requests.get(q).json()\n",
    "    # Based on preliminary parsing, get the list of article from the file\n",
    "    articleList = r['response']['docs']\n",
    "    # Return dataframe\n",
    "    frame = getRequest(articleList)\n",
    "    # Add to dataset\n",
    "    dataset = dataset.append(frame, ignore_index=True)\n",
    "    # Create csv file\n",
    "    csv_path = \"Asians American NYT Dataset.csv\"\n",
    "    dataset.to_csv(csv_path, index=False)\n",
    "    return dataset\n",
    "\n",
    "getFinalPage(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TODO TASKS\n",
    "*[7.20] Meeting with my supervisor to discuss the dataset so far and the next steps to be taken*\n",
    "- Clean the data for duplicates and empty lines\n",
    "- Reorganize keywords with one-hot encoding for specific subject (hate crime, discrimination, coronavirus, etc)\n",
    "- Drop unnecessary column, filter date range to start with the same date on NYT Covid dataset\n",
    "- Explore the dataset with the news-desk count\n",
    "- Aggregate by date with the sum of the specific subject, article count before merging\n",
    "- Download NYT Covid dataset with dates\n",
    "- Finalize merged dataset with the date\n",
    "- Distribution curve and general exploratory tasks"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}